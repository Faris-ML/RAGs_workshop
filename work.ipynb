{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\tfgam\\.conda\\envs\\test\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install openai\n",
    "# ! pip install chromadb\n",
    "# ! pip install pypdf\n",
    "# ! pip install tiktoken\n",
    "# ! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the vector store directory and the document path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_dir = 'vector_store'\n",
    "document_path = '2312.16171v1 (1).pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the document and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = PyPDFLoader(document_path).load()\n",
    "splitted_doc = splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the chromadb vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(persist_directory=vector_store_dir,embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x156e5d94a60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.from_documents(splitted_doc,\n",
    "                            OpenAIEmbeddings(),\n",
    "                            persist_directory=vector_store_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LLM and start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TFgam\\.conda\\envs\\test\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(temperature=0,model_name = 'gpt-4-1106-preview')\n",
    "retrival = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = RetrievalQAWithSourcesChain.from_llm(llm=model,retriever=retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt instructions are as follows:\n",
      "\n",
      "1. Start with ‘###Instruction###’, followed by either ‘###Example###’ or ‘###Question###’ if relevant.\n",
      "2. Present your content with conciseness and clarity, avoiding verbosity or ambiguity.\n",
      "3. Provide contextual relevance by including keywords, domain-specific terminology, or situational descriptions.\n",
      "4. Ensure task alignment by using language and structure that clearly indicate the nature of the task.\n",
      "5. Include example demonstrations for complex tasks, showing input-output pairs if necessary.\n",
      "6. Avoid bias by using neutral language and considering ethical implications.\n",
      "7. Use incremental prompting for tasks that require a sequence of steps, breaking down the task into a series of prompts.\n",
      "8. Adjust prompts based on the model's performance and iterative feedback.\n",
      "9. For advanced tasks, incorporate programming-like logic such as conditional statements or logical operators.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "what is the prompt instructinos ?\n",
    "\"\"\"\n",
    "question = {\"question\":  question}\n",
    "res = chatbot(question)\n",
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"\"\"\n",
    "السلام عليكم, كيف حالك اليوم ؟\n",
    "\"\"\"\n",
    "text_2 = \"\"\"\n",
    "السلام عليكم, كيف حالك اليوم ؟\n",
    "\"\"\"\n",
    "emb_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1 = np.array(emb_model.embed_query(text_1))\n",
    "emb_2 = np.array(emb_model.embed_query(text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (np.dot(emb_1,emb_2))/(np.linalg.norm(emb_1)*np.linalg.norm(emb_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
